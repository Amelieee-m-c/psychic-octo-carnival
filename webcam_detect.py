import cv2
import time
import os
import threading
import pyttsx3 
from ultralytics import YOLO

# ================= ğŸ”§ åƒæ•¸èª¿æ•´å€ (æ§åˆ¶å°) =================
MODEL_PATH = "best.pt"
SAVE_FOLDER = "tongue_captures"

# 1. ä¿¡å¿ƒåº¦é–€æª»
CONF_THRESHOLD = 0.7 

# 2. è·é›¢/é¢ç©é–€æª» (ç•«é¢ä½”æ¯”)
SIZE_MIN = 0.10  # å¤ªé 
SIZE_MAX = 0.70  # å¤ªè¿‘/èª¤åˆ¤

# 3. å½¢ç‹€éæ¿¾ (é•·å¯¬æ¯”)
AR_MIN = 0.6 
AR_MAX = 1.6

# 4. å€’æ•¸ç§’æ•¸
COUNTDOWN_SEC = 3

# 5. ğŸ†• é‚Šç·£ä¿ç•™å€ (åƒç´ )
# å¦‚æœèˆŒé ­é›¢é‚Šç·£å°æ–¼é€™å€‹è·é›¢ï¼Œå°±è¦–ç‚ºè¢«è£åˆ‡
MARGIN = 15 
# ========================================================

# --- åˆå§‹åŒ–èªéŸ³ ---
def speak(text):
    def _speak_thread():
        try:
            eng = pyttsx3.init() 
            eng.setProperty('rate', 150)
            eng.say(text)
            eng.runAndWait()
        except:
            pass
    threading.Thread(target=_speak_thread).start()

# --- å»ºç«‹è³‡æ–™å¤¾ ---
if not os.path.exists(SAVE_FOLDER):
    os.makedirs(SAVE_FOLDER)

# --- è¼‰å…¥æ¨¡å‹ ---
print(f"ğŸ” æª¢æŸ¥æ¨¡å‹è·¯å¾‘: {os.path.abspath(MODEL_PATH)}")
if os.path.exists(MODEL_PATH):
    print(f"æ‰¾åˆ°æ¨¡å‹: {MODEL_PATH}")
    try:
        model = YOLO(MODEL_PATH)
    except Exception as e:
        print(f"è¼‰å…¥å¤±æ•—: {e}, æ”¹ç”¨ yolov8n.pt")
        model = YOLO("yolov8n.pt")
else:
    print(f"!!! æ‰¾ä¸åˆ° {MODEL_PATH}ï¼Œç³»çµ±åˆ‡æ›è‡³ yolov8n.pt")
    model = YOLO("yolov8n.pt")

# --- é–‹å•Ÿæ”å½±æ©Ÿ ---
cap = cv2.VideoCapture(0, cv2.CAP_MSMF)
FONT = cv2.FONT_HERSHEY_SIMPLEX

# --- ç‹€æ…‹è®Šæ•¸ ---
start_time = 0
counting = False
last_spoken_count = COUNTDOWN_SEC + 1
last_instruction_time = 0
current_status = "idle" 

print("ğŸŸ¢ ç¨‹å¼å•Ÿå‹•ï¼Œè«‹å°‡èˆŒé ­å°æº–æ¡†æ¡†")
speak("System Ready")

while cap.isOpened():
    success, frame = cap.read()
    if not success: 
        print("ç„¡æ³•è®€å–é¡é ­")
        break

    # 1. é¡åƒç¿»è½‰
    frame = cv2.flip(frame, 1)
    clean_frame = frame.copy()
    
    frame_h, frame_w = frame.shape[:2]
    frame_area = frame_h * frame_w

    # 2. AI é æ¸¬
    results = model.predict(frame, verbose=False, conf=CONF_THRESHOLD)
    
    is_good_frame = False 
    
    for result in results:
        boxes = result.boxes
        for box in boxes:
            x1, y1, x2, y2 = map(int, box.xyxy[0])
            
            # ç¢ºä¿åº§æ¨™åœ¨ç•«é¢å…§
            x1, y1 = max(0, x1), max(0, y1)
            x2, y2 = min(frame_w, x2), min(frame_h, y2)

            # =========================================================
            # ğŸ†• æ–°å¢é‚è¼¯ï¼šé‚Šç·£è§¸ç¢°æª¢æŸ¥ (Border/Edge Check)
            # =========================================================
            touching_edge = (
                x1 < MARGIN or              # å¤ªé å·¦
                y1 < MARGIN or              # å¤ªé ä¸Š
                x2 > frame_w - MARGIN or    # å¤ªé å³
                y2 > frame_h - MARGIN       # å¤ªé ä¸‹
            )

            if touching_edge:
                # ğŸ”´ è§¸é‚Šè™•ç†ï¼šé¡¯ç¤ºç´…æ¡†ã€é‡ç½®å€’æ•¸ã€èªéŸ³æç¤º
                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)
                cv2.putText(frame, "CUT OFF!", (x1, y1 - 35), FONT, 0.8, (0, 0, 255), 2)
                cv2.putText(frame, "CENTER IT", (x1, y1 - 10), FONT, 0.8, (0, 0, 255), 2)
                
                counting = False # é‡ç½®å€’æ•¸
                
                # èªéŸ³æç¤º (æ¯ 3 ç§’ä¸€æ¬¡)
                if time.time() - last_instruction_time > 3:
                     speak("Center your tongue") 
                     last_instruction_time = time.time()
                
                continue # â›” è·³éé€™æ¬¡è¿´åœˆ (ä¸é€²è¡Œå¾Œé¢çš„åˆæ ¼åˆ¤æ–·)
            # =========================================================

            # è¨ˆç®—å¹¾ä½•ç‰¹å¾µ
            w = x2 - x1
            h = y2 - y1
            box_area = w * h
            ratio = box_area / frame_area
            aspect_ratio = w / h if h > 0 else 0

            # --- éæ¿¾å™¨ ---
            
            # éæ¿¾ 1: å½¢ç‹€ä¸å°
            if aspect_ratio < AR_MIN or aspect_ratio > AR_MAX:
                cv2.rectangle(frame, (x1, y1), (x2, y2), (100, 100, 100), 1)
                continue 

            # éæ¿¾ 2: é¢ç©å¤ªå¤§ (é›–ç„¶æœ‰é‚Šç·£æª¢æŸ¥ï¼Œä½†å¤ªæ»¿ä¹Ÿå¯èƒ½æ˜¯èª¤åˆ¤)
            if ratio > SIZE_MAX:
                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)
                cv2.putText(frame, "Too Close", (x1, y1-10), FONT, 0.8, (0,0,255), 2)
                counting = False
                continue

            # --- è·é›¢åˆ¤æ–·èˆ‡å€’æ•¸é‚è¼¯ ---
            now = time.time()

            if ratio < SIZE_MIN:
                # å¤ªé 
                color = (0, 0, 255)
                label = f"Too Far ({ratio:.1%})"
                instruction = "MOVE CLOSER"
                counting = False 
                
                if now - last_instruction_time > 3:
                    speak("Move Closer")
                    last_instruction_time = now

            else:
                # âœ… åˆæ ¼ -> å€’æ•¸
                color = (0, 255, 0)
                label = f"Good ({ratio:.1%})"
                is_good_frame = True
                
                if not counting:
                    counting = True
                    start_time = now
                    last_spoken_count = COUNTDOWN_SEC + 1
                    instruction = "HOLD STILL"
                    speak("Hold still")
                else:
                    elapsed = now - start_time
                    remaining = COUNTDOWN_SEC - elapsed
                    current_count_int = int(remaining) + 1

                    if remaining > 0:
                        instruction = f"Wait... {current_count_int}"
                        # ä¸­å¤®å¤§å­—å€’æ•¸
                        cv2.putText(frame, str(current_count_int), 
                                    (int(frame_w/2)-30, int(frame_h/2)), 
                                    FONT, 4, (0, 255, 255), 5)
                        
                        if current_count_int < last_spoken_count:
                            speak(str(current_count_int))
                            last_spoken_count = current_count_int
                            
                    else:
                        # ğŸ“¸ æ‹ç…§
                        instruction = "CAPTURED!"
                        speak("Captured") 
                        
                        filename = f"{SAVE_FOLDER}/tongue_{int(time.time())}.jpg"
                        roi_img = clean_frame[y1:y2, x1:x2]
                        
                        if roi_img.size > 0:
                            cv2.imwrite(filename, roi_img)
                            print(f"ğŸ“¸ å·²å­˜æª”: {filename}")
                            cv2.rectangle(frame, (0, 0), (frame_w, frame_h), (255, 255, 255), -1)
                        
                        counting = False
                        time.sleep(1) 

            # ç•«å‡ºæ¡†æ¡†èˆ‡è³‡è¨Š
            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 3)
            cv2.putText(frame, label, (x1, y1 - 35), FONT, 0.6, color, 2)
            cv2.putText(frame, instruction, (x1, y1 - 10), FONT, 0.8, color, 2)

    # ç„¡äºº/ä¸åˆæ ¼ç‹€æ…‹é‡ç½®
    if not is_good_frame:
        counting = False
        if time.time() - last_instruction_time > 10 and time.time() - start_time > 10:
             last_instruction_time = time.time()

    cv2.imshow("Smart Capture", frame)
    if cv2.waitKey(1) & 0xFF == ord('q'): break

cap.release()
cv2.destroyAllWindows()